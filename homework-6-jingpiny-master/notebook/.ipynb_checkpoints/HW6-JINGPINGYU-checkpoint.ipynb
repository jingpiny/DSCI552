{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782118ce",
   "metadata": {},
   "source": [
    "<b> JINGPING YU    id: 6205835871"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4ab889",
   "metadata": {},
   "source": [
    "1. Supervised, Semi-Supervised, and Unsupervised Learning\n",
    "(b) Monte-Carlo Simulation: Repeat the following procedures for supervised, unsupervised, and semi-supervised learning M = 30 times, and use randomly selected train and test data (make sure you use 20% of both the positve and negative classes as the test set). Then compare the average scores (accuracy, precision, recall, F1-score, and AUC) that you obtain from each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35de7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee56fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3       4       5        6        7        8        9   \\\n",
       "0     1  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
       "1     1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
       "2     1  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
       "3     1  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
       "4     1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
       "..   ..    ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "564   1  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n",
       "565   1  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n",
       "566   1  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n",
       "567   1  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n",
       "568   0   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n",
       "\n",
       "         10  ...      22     23      24      25       26       27      28  \\\n",
       "0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "data = pd.read_csv('../data/wdbc.data', header = None)\n",
    "# get rid of the first column\n",
    "data = data.iloc[:, 1:]\n",
    "# binary encoding: replace B with 0 and M with 1\n",
    "data.replace('B', 0, inplace=True)\n",
    "data.replace('M', 1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4268cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.540</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.12880</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9.504</td>\n",
       "      <td>12.44</td>\n",
       "      <td>60.34</td>\n",
       "      <td>273.9</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.06492</td>\n",
       "      <td>0.02956</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.1815</td>\n",
       "      <td>...</td>\n",
       "      <td>10.230</td>\n",
       "      <td>15.66</td>\n",
       "      <td>65.13</td>\n",
       "      <td>314.9</td>\n",
       "      <td>0.13240</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.08867</td>\n",
       "      <td>0.06227</td>\n",
       "      <td>0.2450</td>\n",
       "      <td>0.07773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>13.030</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.02562</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.04833</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8.196</td>\n",
       "      <td>16.84</td>\n",
       "      <td>51.71</td>\n",
       "      <td>201.9</td>\n",
       "      <td>0.08600</td>\n",
       "      <td>0.05943</td>\n",
       "      <td>0.01588</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>...</td>\n",
       "      <td>8.964</td>\n",
       "      <td>21.96</td>\n",
       "      <td>57.26</td>\n",
       "      <td>242.2</td>\n",
       "      <td>0.12970</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.06880</td>\n",
       "      <td>0.02564</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.07409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0</td>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.10290</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0</td>\n",
       "      <td>11.510</td>\n",
       "      <td>23.93</td>\n",
       "      <td>74.52</td>\n",
       "      <td>403.5</td>\n",
       "      <td>0.09261</td>\n",
       "      <td>0.10210</td>\n",
       "      <td>0.11120</td>\n",
       "      <td>0.041050</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>...</td>\n",
       "      <td>12.480</td>\n",
       "      <td>37.16</td>\n",
       "      <td>82.28</td>\n",
       "      <td>474.2</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.25170</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.09653</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.08732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0</td>\n",
       "      <td>14.050</td>\n",
       "      <td>27.15</td>\n",
       "      <td>91.38</td>\n",
       "      <td>600.4</td>\n",
       "      <td>0.09929</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>...</td>\n",
       "      <td>15.300</td>\n",
       "      <td>33.17</td>\n",
       "      <td>100.20</td>\n",
       "      <td>706.7</td>\n",
       "      <td>0.12410</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.13260</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.08321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0</td>\n",
       "      <td>11.200</td>\n",
       "      <td>29.37</td>\n",
       "      <td>70.67</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.07449</td>\n",
       "      <td>0.03558</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920</td>\n",
       "      <td>38.30</td>\n",
       "      <td>75.19</td>\n",
       "      <td>439.6</td>\n",
       "      <td>0.09267</td>\n",
       "      <td>0.05494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0</td>\n",
       "      <td>7.760</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1       2      3      4      5        6        7        8         9   \\\n",
       "0     0  13.540  14.36  87.46  566.3  0.09779  0.08129  0.06664  0.047810   \n",
       "1     0  13.080  15.71  85.63  520.0  0.10750  0.12700  0.04568  0.031100   \n",
       "2     0   9.504  12.44  60.34  273.9  0.10240  0.06492  0.02956  0.020760   \n",
       "3     0  13.030  18.42  82.61  523.8  0.08983  0.03766  0.02562  0.029230   \n",
       "4     0   8.196  16.84  51.71  201.9  0.08600  0.05943  0.01588  0.005917   \n",
       "..   ..     ...    ...    ...    ...      ...      ...      ...       ...   \n",
       "352   0  14.590  22.68  96.39  657.1  0.08473  0.13300  0.10290  0.037360   \n",
       "353   0  11.510  23.93  74.52  403.5  0.09261  0.10210  0.11120  0.041050   \n",
       "354   0  14.050  27.15  91.38  600.4  0.09929  0.11260  0.04462  0.043040   \n",
       "355   0  11.200  29.37  70.67  386.0  0.07449  0.03558  0.00000  0.000000   \n",
       "356   0   7.760  24.54  47.92  181.0  0.05263  0.04362  0.00000  0.000000   \n",
       "\n",
       "         10  ...      22     23      24     25       26       27       28  \\\n",
       "0    0.1885  ...  15.110  19.26   99.70  711.2  0.14400  0.17730  0.23900   \n",
       "1    0.1967  ...  14.500  20.49   96.09  630.5  0.13120  0.27760  0.18900   \n",
       "2    0.1815  ...  10.230  15.66   65.13  314.9  0.13240  0.11480  0.08867   \n",
       "3    0.1467  ...  13.300  22.81   84.46  545.9  0.09701  0.04619  0.04833   \n",
       "4    0.1769  ...   8.964  21.96   57.26  242.2  0.12970  0.13570  0.06880   \n",
       "..      ...  ...     ...    ...     ...    ...      ...      ...      ...   \n",
       "352  0.1454  ...  15.480  27.27  105.90  733.5  0.10260  0.31710  0.36620   \n",
       "353  0.1388  ...  12.480  37.16   82.28  474.2  0.12980  0.25170  0.36300   \n",
       "354  0.1537  ...  15.300  33.17  100.20  706.7  0.12410  0.22640  0.13260   \n",
       "355  0.1060  ...  11.920  38.30   75.19  439.6  0.09267  0.05494  0.00000   \n",
       "356  0.1587  ...   9.456  30.37   59.16  268.6  0.08996  0.06444  0.00000   \n",
       "\n",
       "          29      30       31  \n",
       "0    0.12880  0.2977  0.07259  \n",
       "1    0.07283  0.3184  0.08183  \n",
       "2    0.06227  0.2450  0.07773  \n",
       "3    0.05013  0.1987  0.06169  \n",
       "4    0.02564  0.3105  0.07409  \n",
       "..       ...     ...      ...  \n",
       "352  0.11050  0.2258  0.08004  \n",
       "353  0.09653  0.2112  0.08732  \n",
       "354  0.10480  0.2250  0.08321  \n",
       "355  0.00000  0.1566  0.05905  \n",
       "356  0.00000  0.2871  0.07039  \n",
       "\n",
       "[357 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class: Malignant\n",
    "pos = data[data[1] == 1].reset_index(drop=True)\n",
    "# class: Benigh\n",
    "neg = data[data[1] == 0].reset_index(drop=True)\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8309a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c355727",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function: train test split\n",
    "parameter: \n",
    "1. positive class\n",
    "2. negative class\n",
    "3. split ratio\n",
    "return:\n",
    "xtrain, xtest, ytrain, ytest\n",
    "'''\n",
    "\n",
    "def data_split(positive, negative, ratio, i):\n",
    "    # randomly select 20% of positive class and 80% of positive class\n",
    "    xtrainp, xtestp, ytrainp, ytestp = train_test_split(positive.iloc[:, 1:], positive.iloc[:, 0], test_size = ratio, random_state=i)\n",
    "    # randomly select 20% of negative class and 80% of negative class\n",
    "    xtrainn, xtestn, ytrainn, ytestn = train_test_split(negative.iloc[:, 1:], negative.iloc[:, 0], test_size = ratio, random_state=i)\n",
    "    \n",
    "    # concatenate positive with negative\n",
    "    xtrain = pd.concat([xtrainp,xtrainn], ignore_index=True)\n",
    "    xtest = pd.concat([xtestp,xtestn], ignore_index=True)\n",
    "    ytrain = pd.concat([ytrainp, ytrainn], ignore_index=True)\n",
    "    ytest = pd.concat([ytestp, ytestn], ignore_index=True)\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "241d8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "'''\n",
    "function: perform normalization\n",
    "parameter:\n",
    "1. xtrain\n",
    "2. xtest\n",
    "return:\n",
    "scalered xtrain, scalered xtest\n",
    "'''\n",
    "def scaler(xtrain, xtest):\n",
    "    xtrain_std = MinMaxScaler().fit_transform(xtrain)\n",
    "    xtest_std = MinMaxScaler().fit_transform(xtest)\n",
    "    return xtrain_std, xtest_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad02c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function: calculate statistics\n",
    "parameter:\n",
    "1.  true y\n",
    "2. predicted y\n",
    "3. the probability of y\n",
    "return accuracy, precision, recall, f1-score, auc\n",
    "'''\n",
    "def stats(ytrue, ypred, yprob):\n",
    "    acc = accuracy_score(ytrue, ypred)\n",
    "    pre = precision_score(ytrue, ypred, pos_label=1)\n",
    "    recall = recall_score(ytrue, ypred, pos_label=1)\n",
    "    f1 = f1_score(ytrue, ypred, pos_label=1, average='weighted')\n",
    "    auc = roc_auc_score(ytrue, yprob)\n",
    "    \n",
    "    return acc, pre, recall, f1, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b149e0",
   "metadata": {},
   "source": [
    "i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e28919a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "# return average accuracy, precision, F1-score and AUC\n",
    "def supervised(xtrain, ytrain, xtest, ytest):\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    \n",
    "    # use cross validation to find the best C parameter\n",
    "    clf = GridSearchCV(svc, parameters, cv=5).fit(xtrain, ytrain)\n",
    "    best_C = clf.best_params_['C']\n",
    "    \n",
    "    # refit best C into a new model\n",
    "    svc1 = LinearSVC(penalty='l1', dual=False, C=best_C).fit(xtrain, ytrain)\n",
    "    \n",
    "    # predict y for both trainning and test sets using the refitted model\n",
    "    ypred = svc1.predict(xtrain)\n",
    "    ypred_test = svc1.predict(xtest)\n",
    "    \n",
    "    yprob = svc1.decision_function(xtrain)\n",
    "    yprob_test = svc1.decision_function(xtest)\n",
    "\n",
    "    # calculate the statistics for training and testing sets\n",
    "    acc1, pre1, recall1, f11, auc1 = stats(ytrain, ypred, yprob)\n",
    "    acc2, pre2, recall2, f12, auc2 = stats(ytest, ypred_test, yprob_test)\n",
    "\n",
    "    return acc1, acc2, pre1, pre2, recall1, recall2, f11, f12, auc1, auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fd7e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Accuracy: 0.9853891336270191\n",
      "Average Test Accuracy: 0.9118840579710145\n",
      "Average Train Precision: 0.9913284952515173\n",
      "Average Test Precision: 0.839550710093278\n",
      "Average Train Recall: 0.9692307692307692\n",
      "Average Test Recall: 0.9798449612403101\n",
      "Average Train F1-score: 0.9853522606384902\n",
      "Average Test F1-score: 0.9127320800981937\n",
      "Average Train AUC: 0.997619986850756\n",
      "Average Test AUC: 0.991365202411714\n"
     ]
    }
   ],
   "source": [
    "# define the parameter range\n",
    "paramC = np.logspace(-3,3,7)\n",
    "parameters = {'C':paramC}\n",
    "train_acc, test_acc, train_pre, test_pre, train_re, test_re, train_f1, test_f1, train_auc, test_auc = ([] for i in range(10))\n",
    "\n",
    "for i in range(30):\n",
    "    \n",
    "    # split the positive and negative classes and normalize the data\n",
    "    xtrain,xtest,ytrain,ytest = data_split(pos, neg, 0.2, i)\n",
    "    xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "    \n",
    "    # train the algorithm\n",
    "    acc1, acc2, pre1, pre2, recall1, recall2, f11, f12, auc1, auc2 = supervised(xtrain_norm, ytrain, xtest_norm, ytest)\n",
    "    # add the statistical results into the lists\n",
    "    train_acc.append(acc1)\n",
    "    test_acc.append(acc2)\n",
    "    \n",
    "    train_pre.append(pre1)\n",
    "    test_pre.append(pre2)\n",
    "    \n",
    "    train_re.append(recall1)\n",
    "    test_re.append(recall2)\n",
    "    \n",
    "    train_f1.append(f11)\n",
    "    test_f1.append(f12)\n",
    "    \n",
    "    train_auc.append(auc1)\n",
    "    test_auc.append(auc2)\n",
    "\n",
    "print(\"Average Train Accuracy:\", mean(train_acc))\n",
    "print(\"Average Test Accuracy:\", mean(test_acc))\n",
    "print(\"Average Train Precision:\", mean(train_pre))\n",
    "print(\"Average Test Precision:\", mean(test_pre))\n",
    "print(\"Average Train Recall:\", mean(train_re))\n",
    "print(\"Average Test Recall:\", mean(test_re))\n",
    "print(\"Average Train F1-score:\", mean(train_f1))\n",
    "print(\"Average Test F1-score:\", mean(test_f1))\n",
    "print(\"Average Train AUC:\", mean(train_auc))\n",
    "print(\"Average Test AUC:\", mean(test_auc))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9bc48",
   "metadata": {},
   "source": [
    "Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b947c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion Matrix:\n",
      " [[284   1]\n",
      " [  5 164]]\n",
      "Test Confusion Matrix:\n",
      " [[63  9]\n",
      " [ 1 42]]\n"
     ]
    }
   ],
   "source": [
    "# pick the last simulation to plot: i = 29\n",
    "xtrain,xtest,ytrain,ytest = data_split(pos, neg, 0.2, 29)\n",
    "xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "\n",
    "# use cross validation to find the best C parameter\n",
    "clf = GridSearchCV(svc, parameters, cv=5).fit(xtrain_norm, ytrain)\n",
    "best_C = clf.best_params_['C']\n",
    "\n",
    "# refit best C into a new model\n",
    "svc1 = LinearSVC(penalty='l1', dual=False, C=best_C).fit(xtrain_norm, ytrain)\n",
    "\n",
    "# predict y for both trainning and test sets using the refitted model\n",
    "ypred = svc1.predict(xtrain_norm)\n",
    "ypred_test = svc1.predict(xtest_norm)\n",
    "\n",
    "# report the confusion_matrix for both training and test sets\n",
    "train_cm = confusion_matrix(ytrain, ypred)\n",
    "test_cm = confusion_matrix(ytest, ypred_test)\n",
    "print('\\nTrain Confusion Matrix:\\n', train_cm)\n",
    "print('Test Confusion Matrix:\\n', test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "448a0a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yprob = svc1.decision_function(xtrain_norm)\n",
    "yprob_test = svc1.decision_function(xtest_norm)\n",
    "# plot the ROC for simulation 29\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(ytrain, yprob)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(ytest, yprob_test)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "plt.title(\"ROC Curve for Last Simulation\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b533dbae",
   "metadata": {},
   "source": [
    "ii. Semi-Supervised Learning/ Self-training: select 50% of the positive class along with 50% of the negative class in the training set as labeled data and the rest as unlabelled data. You can select them randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb7600",
   "metadata": {},
   "source": [
    "A. Train an L1-penalized SVM to classify the labeled data Use normalized data. Choose the penalty parameter using 5 fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9d911",
   "metadata": {},
   "source": [
    "B. Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it (ignore its true label), and add it to the labeled data, and retrain the SVM. Continue this process until all unlabeled data are used. Test the final SVM on the test data andthe average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "52f87d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised(xtrain, ytrain, xtest, ytest, xtrain_unlabel):\n",
    "    svc = LinearSVC(penalty='l1', dual=False)\n",
    "    clf = GridSearchCV(svc, parameters, cv=5).fit(xtrain, ytrain)\n",
    "    best_C = clf.best_params_['C']\n",
    "    \n",
    "    # refit best C to a new model\n",
    "    svc1 = LinearSVC(penalty='l1', dual=False, C=best_C).fit(xtrain, ytrain)\n",
    "    \n",
    "    # find the farthests unlabeled data\n",
    "    while len(xtrain_unlabel) > 0:\n",
    "        # find the distance between data and boundary\n",
    "        dist = svc1.decision_function(xtrain_unlabel)\n",
    "        \n",
    "        # find the index of farthest data\n",
    "        dist_abs = np.abs(dist)\n",
    "        index = dist_abs.argmax()\n",
    "        # if the distance is negative, svc predicts it as 0, if the distance is positive, svc predicts it as 1\n",
    "        # negative: left side, positive: right side\n",
    "        if dist[index] < 0:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        \n",
    "        # add the farthest unlabel data to the label data\n",
    "        xtrain = np.append(xtrain, [xtrain_unlabel.iloc[index]], axis = 0)\n",
    "        \n",
    "        # add the predicted label to the label data\n",
    "        ytrain = ytrain.append(pd.Series(label))\n",
    "        \n",
    "        # delete the farthest data from the unlabel set\n",
    "        xtrain_unlabel = xtrain_unlabel.drop(xtrain_unlabel.index[index])\n",
    "        \n",
    "        # retrain the SVC model using the new training set\n",
    "        svc1 = LinearSVC(penalty='l1', dual=False, C=best_C).fit(xtrain, ytrain)\n",
    "    \n",
    "    # predict y for both trainning and test sets using the refitted model\n",
    "    ypred = svc1.predict(xtrain)\n",
    "    ypred_test = svc1.predict(xtest)\n",
    "    \n",
    "    yprob = svc1.decision_function(xtrain)\n",
    "    yprob_test = svc1.decision_function(xtest)\n",
    "\n",
    "    # calculate the statistics for training and testing sets\n",
    "    acc1, pre1, recall1, f11, auc1 = stats(ytrain, ypred, yprob)\n",
    "    acc2, pre2, recall2, f12, auc2 = stats(ytest, ypred_test, yprob_test)\n",
    "\n",
    "    return acc1, acc2, pre1, pre2, recall1, recall2, f11, f12, auc1, auc2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cb20c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Average Train Accuracy: 0.993538913362702\n",
      "Average Test Accuracy: 0.9159420289855073\n",
      "Average Train Precision: 0.9961375750182763\n",
      "Average Test Precision: 0.8518819523750486\n",
      "Average Train Recall: 0.9861140770566347\n",
      "Average Test Recall: 0.9720930232558139\n",
      "Average Train F1-score: 0.9935293177705662\n",
      "Average Test F1-score: 0.9137990145590851\n",
      "Average Train AUC: 0.99948494839606\n",
      "Average Test AUC: 0.9864341085271318\n"
     ]
    }
   ],
   "source": [
    "# normalize the xtrain and xtest\n",
    "xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "# concatenate the training sets\n",
    "train = pd.concat([ytrain,pd.DataFrame(xtrain_norm, columns=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31])], axis=1)\n",
    "\n",
    "train_acc, test_acc, train_pre, test_pre, train_re, test_re, train_f1, test_f1, train_auc, test_auc = ([] for i in range(10))\n",
    "\n",
    "# Monte-Carlo\n",
    "for i in range(30):\n",
    "    print(i,end=\" \")\n",
    "\n",
    "    # divide 50% of positive class and 50% of negative class in the training set as label data\n",
    "    xtrain_label, xtrain_unlabel, ytrain_label, ytrain_unlabel = data_split(train[train[1]==1], train[train[1]==0], 0.5, i)\n",
    "    \n",
    "    # train the algorithm\n",
    "    acc1, acc2, pre1, pre2, recall1, recall2, f11, f12, auc1, auc2 = semi_supervised(xtrain_label, ytrain_label, xtest_norm, ytest, xtrain_unlabel)\n",
    "    # add the statistical results into the lists\n",
    "    train_acc.append(acc1)\n",
    "    test_acc.append(acc2)\n",
    "    \n",
    "    train_pre.append(pre1)\n",
    "    test_pre.append(pre2)\n",
    "    \n",
    "    train_re.append(recall1)\n",
    "    test_re.append(recall2)\n",
    "    \n",
    "    train_f1.append(f11)\n",
    "    test_f1.append(f12)\n",
    "    \n",
    "    train_auc.append(auc1)\n",
    "    test_auc.append(auc2)\n",
    "\n",
    "print(\"Average Train Accuracy:\", mean(train_acc))\n",
    "print(\"Average Test Accuracy:\", mean(test_acc))\n",
    "print(\"Average Train Precision:\", mean(train_pre))\n",
    "print(\"Average Test Precision:\", mean(test_pre))\n",
    "print(\"Average Train Recall:\", mean(train_re))\n",
    "print(\"Average Test Recall:\", mean(test_re))\n",
    "print(\"Average Train F1-score:\", mean(train_f1))\n",
    "print(\"Average Test F1-score:\", mean(test_f1))\n",
    "print(\"Average Train AUC:\", mean(train_auc))\n",
    "print(\"Average Test AUC:\", mean(test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740cdac",
   "metadata": {},
   "source": [
    "Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71dc9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion Matrix:\n",
      " [[204  81]\n",
      " [ 87  82]]\n",
      "Test Confusion Matrix:\n",
      " [[64  8]\n",
      " [ 2 41]]\n"
     ]
    }
   ],
   "source": [
    "# pick the last simulation to plot: i = 29\n",
    "xtrain_label, xtrain_unlabel, ytrain_label, ytrain_unlabel = data_split(train[train[1]==1], train[train[1]==0], 0.5, 29)\n",
    "\n",
    "svc = LinearSVC(penalty='l1', dual=False)\n",
    "clf = GridSearchCV(svc, parameters, cv=5).fit(xtrain_label, ytrain_label)\n",
    "best_C = clf.best_params_['C']\n",
    "\n",
    "# refit best C to a new model\n",
    "svc1 = LinearSVC(penalty='l1', dual=False, C=best_C).fit(xtrain_label, ytrain_label)\n",
    "\n",
    "# find the farthests unlabeled data\n",
    "while len(xtrain_unlabel) > 0:\n",
    "    # find the distance between data and boundary\n",
    "    dist = svc1.decision_function(xtrain_unlabel)\n",
    "\n",
    "    # find the index of farthest data\n",
    "    dist_abs = np.abs(dist)\n",
    "    index = dist_abs.argmax()\n",
    "    # if the distance is negative, svc predicts it as 0, if the distance is positive, svc predicts it as 1\n",
    "    # negative: left side, positive: right side\n",
    "    if dist[index] < 0:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "\n",
    "    # add the farthest unlabel data to the label data\n",
    "    xtrain_label = np.append(xtrain_label, [xtrain_unlabel.iloc[index]], axis = 0)\n",
    "\n",
    "    # add the predicted label to the label data\n",
    "    ytrain_label = ytrain_label.append(pd.Series(label))\n",
    "\n",
    "    # delete the farthest data from the unlabel set\n",
    "    xtrain_unlabel = xtrain_unlabel.drop(xtrain_unlabel.index[index])\n",
    "\n",
    "    # retrain the SVC model using the new training set\n",
    "    svc1 = LinearSVC(penalty='l1', dual=False, C=best_C).fit(xtrain_label, ytrain_label)\n",
    "\n",
    "# predict y for both trainning and test sets using the refitted model\n",
    "ypred = svc1.predict(xtrain_label)\n",
    "ypred_test = svc1.predict(xtest_norm)\n",
    "\n",
    "# report the confusion_matrix for both training and test sets\n",
    "train_cm = confusion_matrix(ytrain, ypred)\n",
    "test_cm = confusion_matrix(ytest, ypred_test)\n",
    "print('\\nTrain Confusion Matrix:\\n', train_cm)\n",
    "print('Test Confusion Matrix:\\n', test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ac358cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yprob = svc1.decision_function(xtrain_label)\n",
    "yprob_test = svc1.decision_function(xtest_norm)\n",
    "# plot the ROC for simulation 29\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(ytrain_label, yprob)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(ytest, yprob_test)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label='Train ROC curve')\n",
    "plt.plot(test_fpr, test_tpr, label='Test ROC curve')\n",
    "plt.title(\"ROC Curve for Last Simulation\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db033c96",
   "metadata": {},
   "source": [
    "iii. Unsupervised Learning: Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2.\n",
    "A. Run the k-means algorithm multiple times. Make sure that you initialize the algoritm randomly. How do you make sure that the algorithm was not trapped in a local minimum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ef8ed",
   "metadata": {},
   "source": [
    "B. Compute the centers of the two clusters and find the closest 30 data points to each center. Read the true labels of those 30 data points and take a majority poll within them. The majority poll becomes the label predicted by k-means for the members of each cluster. Then compare the labels provided by k-means with the true labels of the training data and report the average accuracy, precision, recall, F1-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed003b5",
   "metadata": {},
   "source": [
    "C. Classify test data based on their proximity to the centers of the clusters. Report the average accuracy, precision, recall, F1-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs for the test data.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2c2aba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clus_label(xtrain, ytrain, labels, center):\n",
    "    # transform the datasets into numpy array\n",
    "    xtrain = np.array(xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    # find all the features of the cluster in the datasets\n",
    "    clus = xtrain[labels]\n",
    "    # find all the true labels of the cluster in the original datasets\n",
    "    ytrue = ytrain[labels]\n",
    "    # create an empty numpy array to store the distance\n",
    "    dist = np.empty(0) \n",
    "    for data in clus:\n",
    "        distance = np.linalg.norm(data - center)\n",
    "        # append the distance to the array\n",
    "        dist = np.append(dist, distance)\n",
    "    # find the closest 30 data points to the center\n",
    "    dist_sort = dist.argsort() \n",
    "    ytrue = ytrue[dist_sort[:30]]\n",
    "    # count the number of occurance of each label\n",
    "    ct = np.bincount(ytrue)\n",
    "    # get the most frequently occured label\n",
    "    major_label = np.argmax(ct) \n",
    "    return major_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aa7e01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(ytrain, ypred_train, ytest, ypred_test):\n",
    "    # accuracy\n",
    "    train_accuracy = accuracy_score(ytrain, ypred_train)\n",
    "    test_accuracy = accuracy_score(ytest, ypred_test)\n",
    "    # confusion matrix\n",
    "    train_cm = confusion_matrix(ytrain, ypred_train)\n",
    "    test_cm = confusion_matrix(ytest, ypred_test)\n",
    "    train_tn, train_fp, train_fn, train_tp = train_cm.ravel()\n",
    "    test_tn, test_fp, test_fn, test_tp = test_cm.ravel()\n",
    "    #precision\n",
    "    pre1=train_tp/(train_tp+train_fp)    \n",
    "    pre2=test_tp/(test_tp+test_fp)\n",
    "    # recall\n",
    "    re1=train_tp/(train_tp+train_fn)\n",
    "    re2=test_tp/(test_tp+test_fn)\n",
    "    # f1\n",
    "    f11=2*(pre1*re1)/(pre1+re1)\n",
    "    f12=2*(pre2*re2)/(pre2+re2)\n",
    "    \n",
    "    return acc1, acc2, pre1, pre2, recall1, recall2, f11, f12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b3e71a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def unsupervised(xtrain, ytrain, xtest, ytest, i):\n",
    "    \n",
    "    # train the model\n",
    "    km = KMeans(n_clusters=2, random_state=i).fit(xtrain)\n",
    "    #find all the labels of the model\n",
    "    train_labels = km.labels_\n",
    "    # get the coordinate of two cluster centers\n",
    "    train_centers = km.cluster_centers_\n",
    "    # split the centers into 0 and 1\n",
    "    center0 = train_centers[0]\n",
    "    center1 = train_centers[1]\n",
    "    # split the data based on the model's label\n",
    "    train_clus0 = np.where(train_labels == 0)\n",
    "    train_clus1 = np.where(train_labels == 1)\n",
    "    # find the majority label\n",
    "    clus0 = clus_label(xtrain, ytrain, train_clus0, center0)\n",
    "    clus1 = clus_label(xtrain, ytrain, train_clus1, center1)\n",
    "    # store the labels as predicted labels for training \n",
    "    ypred_train = train_labels\n",
    "    ypred_train[clus0] = clus0\n",
    "    ypred_train[clus1] = clus1    \n",
    "    # find labels for test sets\n",
    "    test_labels = km.predict(xtest)\n",
    "    # split the data based on the labels\n",
    "    test_clus0 = np.where(test_labels == 0)\n",
    "    test_clus1 = np.where(test_labels == 1)\n",
    "    # store the labels as predicted labels for test sets\n",
    "    ypred_test = test_labels\n",
    "    ypred_test[test_clus0] = clus0\n",
    "    ypred_test[test_clus1] = clus1\n",
    "\n",
    "    # calculate the statistics for training and testing sets\n",
    "    acc1, acc2, pre1, pre2, recall1, recall2, f11, f12 = statistic(ytrain, ypred_train, ytest, ypred_test)    \n",
    "    # auc\n",
    "    auc1 = roc_auc_score(ytrain, pd.DataFrame(km.transform(xtrain)).iloc[:,0])\n",
    "    auc2 = roc_auc_score(ytest, pd.DataFrame(km.transform(xtest)).iloc[:,0])\n",
    "    \n",
    "    return acc1, acc2, pre1, pre2, recall1, recall2, f11, f12, auc1, auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "edf8a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
      "\n",
      "Average Train Accuracy: 0.9933920704845814\n",
      "Average Test Accuracy: 0.9304347826086956\n",
      "Average Train Precision: 0.7245280041897463\n",
      "Average Test Precision: 0.991649118810919\n",
      "Average Train Recall: 0.9879518072289156\n",
      "Average Test Recall: 0.9767441860465116\n",
      "Average Train F1-score: 0.5790127888562618\n",
      "Average Test F1-score: 0.7539830957695878\n",
      "Average Train AUC: 0.6253289041143292\n",
      "Average Test AUC: 0.6295542635658915\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc, train_pre, test_pre, train_re, test_re, train_f1, test_f1, train_auc, test_auc = ([] for i in range(10))\n",
    "\n",
    "for i in range(30):\n",
    "    print(i, end=\" \")\n",
    "    # split the positive and negative classes\n",
    "    xtrain,xtest,ytrain,ytest = data_split(pos, neg, 0.2, i)\n",
    "    \n",
    "    # train the algorithm\n",
    "    acc1, acc2, pre1, pre2, recall1, recall2, f11, f12, auc1, auc2 = unsupervised(xtrain, ytrain, xtest, ytest,i)\n",
    "    # add the statistical results into the lists\n",
    "    train_acc.append(acc1)\n",
    "    test_acc.append(acc2)\n",
    "    \n",
    "    train_pre.append(pre1)\n",
    "    test_pre.append(pre2)\n",
    "    \n",
    "    train_re.append(recall1)\n",
    "    test_re.append(recall2)\n",
    "    \n",
    "    train_f1.append(f11)\n",
    "    test_f1.append(f12)\n",
    "    \n",
    "    train_auc.append(auc1)\n",
    "    test_auc.append(auc2)\n",
    "print(\"\\n\")\n",
    "print(\"Average Train Accuracy:\", mean(train_acc))\n",
    "print(\"Average Test Accuracy:\", mean(test_acc))\n",
    "print(\"Average Train Precision:\", mean(train_pre))\n",
    "print(\"Average Test Precision:\", mean(test_pre))\n",
    "print(\"Average Train Recall:\", mean(train_re))\n",
    "print(\"Average Test Recall:\", mean(test_re))\n",
    "print(\"Average Train F1-score:\", mean(train_f1))\n",
    "print(\"Average Test F1-score:\", mean(test_f1))\n",
    "print(\"Average Train AUC:\", mean(train_auc))\n",
    "print(\"Average Test AUC:\", mean(test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "56c9a3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion Matrix:\n",
      " [[285   0]\n",
      " [ 70  99]]\n",
      "Test Confusion Matrix:\n",
      " [[71  1]\n",
      " [13 30]]\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = data_split(pos, neg, 0.2, 29)\n",
    "\n",
    "km = KMeans(n_clusters=2, random_state=i).fit(xtrain)\n",
    "#find all the labels of the model\n",
    "train_labels = km.labels_\n",
    "# get the coordinate of two cluster centers\n",
    "train_centers = km.cluster_centers_\n",
    "# split the centers into 0 and 1\n",
    "center0 = train_centers[0]\n",
    "center1 = train_centers[1]\n",
    "# split the data based on the model's label\n",
    "train_clus0 = np.where(train_labels == 0)\n",
    "train_clus1 = np.where(train_labels == 1)\n",
    "# find the majority label\n",
    "clus0 = clus_label(xtrain, ytrain, train_clus0, center0)\n",
    "clus1 = clus_label(xtrain, ytrain, train_clus1, center1)\n",
    "# store the labels as predicted labels for training \n",
    "ypred_train = train_labels\n",
    "ypred_train[clus0] = clus0\n",
    "ypred_train[clus1] = clus1    \n",
    "# find labels for test sets\n",
    "test_labels = km.predict(xtest)\n",
    "# split the data based on the labels\n",
    "test_clus0 = np.where(test_labels == 0)\n",
    "test_clus1 = np.where(test_labels == 1)\n",
    "# store the labels as predicted labels for test sets\n",
    "ypred_test = test_labels\n",
    "ypred_test[test_clus0] = clus0\n",
    "ypred_test[test_clus1] = clus1\n",
    "\n",
    "train_cm = confusion_matrix(ytrain, ypred_train)\n",
    "test_cm = confusion_matrix(ytest, ypred_test)\n",
    "\n",
    "print('\\nTrain Confusion Matrix:\\n', train_cm)\n",
    "print('Test Confusion Matrix:\\n', test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "876d6538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fp, train_tp, train_thres = roc_curve(ytrain, pd.DataFrame(km.transform(xtrain)).iloc[:,0])\n",
    "test_fp, test_tp, test_thres = roc_curve(ytest, pd.DataFrame(km.transform(xtest)).iloc[:,0])\n",
    "plt.plot(train_fp, train_tp, label='Train ROC')\n",
    "plt.plot(test_fp, test_tp, label='Test ROC')\n",
    "plt.title(\"ROC Curve for 29 Simulation\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bc0c6",
   "metadata": {},
   "source": [
    "iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clustering based on kernels.3 Research what spectral clustering is. Use RBF kernel with gamma=1 or find a gamma for which the two clutsres have the same balance as the one in original data set (if the positive class has p and the negative class has n samples, the two clusters must have p and n members). Do not label data based on their proximity to cluster center, because spectral clustering may give you non-convex clusters . Instead, use fit − predict method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fb0b9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def spec_clus(xtrain, ytrain, xtest, ytest):\n",
    "    # train the model\n",
    "    sc = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit(xtrain)\n",
    "    # get the labels of all the training data\n",
    "    trlabels = sc.labels_\n",
    "    telabels = sc.fit_predict(xtest)\n",
    "    # split the training data based on the label\n",
    "    train_clus0 = np.where(trlabels==0)\n",
    "    test_clus0 = np.where(telabels==0)\n",
    "    train_clus1 = np.where(trlabels==1)\n",
    "    test_clus1 = np.where(telabels==1)\n",
    "    # transform the dataset into numpy array\n",
    "    ytrain = np.array(ytrain)\n",
    "    ytest = np.array(ytest)\n",
    "    # fin the majority of the label in the original dataset\n",
    "    ytrue0 = ytrain[train_clus0]\n",
    "    ytrue1 = ytrain[train_clus1]\n",
    "    ct0 = np.bincount(ytrue0)\n",
    "    ct1 = np.bincount(ytrue1)\n",
    "    clus0 = np.argmax(ct0)\n",
    "    clus1 = np.argmax(ct1)\n",
    "    # predict the labels for both training the test\n",
    "    ypred_train = train_labels\n",
    "    ypred_train[clus0] = clus0\n",
    "    ypred_train[clus1] = clus1\n",
    "    ypred_test = telabels\n",
    "    ypred_test[test_clus0] = clus0\n",
    "    ypred_test[test_clus1] = clus1\n",
    "    \n",
    "    # calculate the statistics for training and testing sets\n",
    "    acc1, acc2, pre1, pre2, recall1, recall2, f11, f12 = statistic(ytrain, ypred_train, ytest, ypred_test)    \n",
    "\n",
    "    \n",
    "    return acc1, acc2, pre1, pre2, recall1, recall2, f11, f12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "af92d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
      "\n",
      "Average Train Accuracy: 0.9933920704845814\n",
      "Average Test Accuracy: 0.9304347826086956\n",
      "Average Train Precision: 1.0\n",
      "Average Test Precision: 0.6489308030004726\n",
      "Average Train Recall: 0.9879518072289156\n",
      "Average Test Recall: 0.9767441860465116\n",
      "Average Train F1-score: 0.7388059701492536\n",
      "Average Test F1-score: 0.5063750345510122\n"
     ]
    }
   ],
   "source": [
    "train_acc, test_acc, train_pre, test_pre, train_re, test_re, train_f1, test_f1, train_auc, test_auc = ([] for i in range(10))\n",
    "\n",
    "for i in range(30):\n",
    "    print(i, end=\" \")\n",
    "    # split the positive and negative classes\n",
    "    xtrain,xtest,ytrain,ytest = data_split(pos, neg, 0.2, i)\n",
    "    xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "    # train the algorithm\n",
    "    acc1, acc2, pre1, pre2, recall1, recall2, f11, f12 = spec_clus(xtrain_norm, ytrain, xtest_norm, ytest)\n",
    "    # add the statistical results into the lists\n",
    "    train_acc.append(acc1)\n",
    "    test_acc.append(acc2)\n",
    "    \n",
    "    train_pre.append(pre1)\n",
    "    test_pre.append(pre2)\n",
    "    \n",
    "    train_re.append(recall1)\n",
    "    test_re.append(recall2)\n",
    "    \n",
    "    train_f1.append(f11)\n",
    "    test_f1.append(f12)\n",
    "    \n",
    "    train_auc.append(auc1)\n",
    "    test_auc.append(auc2)\n",
    "print(\"\\n\")\n",
    "print(\"Average Train Accuracy:\", mean(train_acc))\n",
    "print(\"Average Test Accuracy:\", mean(test_acc))\n",
    "print(\"Average Train Precision:\", mean(train_pre))\n",
    "print(\"Average Test Precision:\", mean(test_pre))\n",
    "print(\"Average Train Recall:\", mean(train_re))\n",
    "print(\"Average Test Recall:\", mean(test_re))\n",
    "print(\"Average Train F1-score:\", mean(train_f1))\n",
    "print(\"Average Test F1-score:\", mean(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "efc2d616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Confusion Matrix:\n",
      " [[285   0]\n",
      " [ 70  99]]\n",
      "Test Confusion Matrix:\n",
      " [[72  0]\n",
      " [17 26]]\n"
     ]
    }
   ],
   "source": [
    "xtrain,xtest,ytrain,ytest = data_split(pos, neg, 0.2, 29)\n",
    "xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "sc = SpectralClustering(n_clusters=2, gamma=1, affinity='rbf').fit(xtrain_norm)\n",
    "# get the labels of all the training data\n",
    "trlabels = sc.labels_\n",
    "telabels = sc.fit_predict(xtest_norm)\n",
    "# split the training data based on the label\n",
    "train_clus0 = np.where(trlabels==0)\n",
    "test_clus0 = np.where(telabels==0)\n",
    "train_clus1 = np.where(trlabels==1)\n",
    "test_clus1 = np.where(telabels==1)\n",
    "# transform the dataset into numpy array\n",
    "ytrain = np.array(ytrain)\n",
    "ytest = np.array(ytest)\n",
    "# fin the majority of the label in the original dataset\n",
    "ytrue0 = ytrain[train_clus0]\n",
    "ytrue1 = ytrain[train_clus1]\n",
    "ct0 = np.bincount(ytrue0)\n",
    "ct1 = np.bincount(ytrue1)\n",
    "clus0 = np.argmax(ct0)\n",
    "clus1 = np.argmax(ct1)\n",
    "# predict the labels for both training the test\n",
    "ypred_train = train_labels\n",
    "ypred_train[clus0] = clus0\n",
    "ypred_train[clus1] = clus1\n",
    "ypred_test = telabels\n",
    "ypred_test[test_clus0] = clus0\n",
    "ypred_test[test_clus1] = clus1\n",
    "\n",
    "train_cm = confusion_matrix(ytrain, ypred_train)\n",
    "test_cm = confusion_matrix(ytest, ypred_test)\n",
    "\n",
    "print('\\nTrain Confusion Matrix:\\n', train_cm)\n",
    "print('Test Confusion Matrix:\\n', test_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b2e97",
   "metadata": {},
   "source": [
    "v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled.One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4673f",
   "metadata": {},
   "source": [
    "<b> We can tell from the results that there is a huge difference in F1-score between the supervised/semi-supervised and the unsupervised/spectral clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a5961",
   "metadata": {},
   "source": [
    "Supervised:<br>\n",
    "Average Train Accuracy: 0.9853891336270191<br>\n",
    "Average Test Accuracy: 0.9118840579710145<br>\n",
    "Average Train Precision: 0.9913284952515173<br>\n",
    "Average Test Precision: 0.839550710093278<br>\n",
    "Average Train Recall: 0.9692307692307692<br>\n",
    "Average Test Recall: 0.9798449612403101<br>\n",
    "Average Train F1-score: 0.9853522606384902<br>\n",
    "Average Test F1-score: 0.9127320800981937<br>\n",
    "Average Train AUC: 0.997619986850756<br>\n",
    "Average Test AUC: 0.991365202411714<br>\n",
    "\n",
    "Semi-supervised:<br>\n",
    "Average Test Accuracy: 0.9159420289855073<br>\n",
    "Average Train Precision: 0.9961375750182763<br>\n",
    "Average Test Precision: 0.8518819523750486<br>\n",
    "Average Train Recall: 0.9861140770566347<br>\n",
    "Average Test Recall: 0.9720930232558139<br>\n",
    "Average Train F1-score: 0.9935293177705662<br>\n",
    "Average Test F1-score: 0.9137990145590851<br>\n",
    "Average Train AUC: 0.99948494839606<br>\n",
    "Average Test AUC: 0.9864341085271318<br>\n",
    "\n",
    "Unsupervised:<br>\n",
    "Average Train Accuracy: 0.9933920704845814<br>\n",
    "Average Test Accuracy: 0.9304347826086956<br>\n",
    "Average Train Precision: 0.7245280041897463<br>\n",
    "Average Test Precision: 0.991649118810919<br>\n",
    "Average Train Recall: 0.9879518072289156<br>\n",
    "Average Test Recall: 0.9767441860465116<br>\n",
    "Average Train F1-score: 0.5790127888562618<br>\n",
    "Average Test F1-score: 0.7539830957695878<br>\n",
    "Average Train AUC: 0.6253289041143292<br>\n",
    "Average Test AUC: 0.6295542635658915<br>\n",
    "\n",
    "Spectral Clustering:<br>\n",
    "Average Train Accuracy: 0.9933920704845814<br>\n",
    "Average Test Accuracy: 0.9304347826086956<br>\n",
    "Average Train Precision: 1.0<br>\n",
    "Average Test Precision: 0.6489308030004726<br>\n",
    "Average Train Recall: 0.9879518072289156<br>\n",
    "Average Test Recall: 0.9767441860465116<br>\n",
    "Average Train F1-score: 0.7388059701492536<br>\n",
    "Average Test F1-score: 0.5063750345510122<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ccbbbe",
   "metadata": {},
   "source": [
    "2. Active Learning Using Support Vector Machines\n",
    "(a) Download the banknote authentication Data Set from: https://archive.ics. uci.edu/ml/datasets/banknote+authentication. Choose 472 data points randomly as the test set, and the remaining 900 points as the training set. This is a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ae91c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the file\n",
    "df = pd.read_csv('../data/data_banknote_authentication.txt', header=None)\n",
    "df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "680a2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :4]\n",
    "y = df['class']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 472/1372, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e6cf8f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.31470</td>\n",
       "      <td>3.666800</td>\n",
       "      <td>-0.696900</td>\n",
       "      <td>-1.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.16200</td>\n",
       "      <td>10.292600</td>\n",
       "      <td>-1.282100</td>\n",
       "      <td>-4.039200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.83680</td>\n",
       "      <td>10.013200</td>\n",
       "      <td>-4.323900</td>\n",
       "      <td>-4.327600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.50680</td>\n",
       "      <td>1.158800</td>\n",
       "      <td>3.924900</td>\n",
       "      <td>0.125850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.15570</td>\n",
       "      <td>2.890800</td>\n",
       "      <td>0.596930</td>\n",
       "      <td>0.798250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.39012</td>\n",
       "      <td>-0.142790</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>0.350840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>-0.94255</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>-0.241920</td>\n",
       "      <td>0.315930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.60050</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>-2.212600</td>\n",
       "      <td>0.097399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>2.01650</td>\n",
       "      <td>-0.252460</td>\n",
       "      <td>5.170700</td>\n",
       "      <td>1.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>-2.07590</td>\n",
       "      <td>10.822300</td>\n",
       "      <td>2.643900</td>\n",
       "      <td>-4.837000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     variance   skewness  curtosis   entropy\n",
       "0    -2.31470   3.666800 -0.696900 -1.247400\n",
       "1     1.16200  10.292600 -1.282100 -4.039200\n",
       "2     4.83680  10.013200 -4.323900 -4.327600\n",
       "3     2.50680   1.158800  3.924900  0.125850\n",
       "4     3.15570   2.890800  0.596930  0.798250\n",
       "..        ...        ...       ...       ...\n",
       "895   0.39012  -0.142790 -0.031994  0.350840\n",
       "896  -0.94255   0.039307 -0.241920  0.315930\n",
       "897   0.60050   0.999450 -2.212600  0.097399\n",
       "898   2.01650  -0.252460  5.170700  1.076300\n",
       "899  -2.07590  10.822300  2.643900 -4.837000\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xtrain = xtrain.reset_index(drop = True)\n",
    "# ytrain = ytrain.reset_index(drop = True)\n",
    "# xtest = xtest.reset_index(drop = True)\n",
    "# ytest = ytest.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a232d1e",
   "metadata": {},
   "source": [
    "(b) Repeat each of the following two procedures 50 times. You will have 50 errors for 90 SVMs per each procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc951da",
   "metadata": {},
   "source": [
    "i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the penalty parameter using 5-fold cross validation.4 Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "fb75f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- with mean test error: 0.9814030131826742\n",
      "1 --- with mean test error: 0.978319209039548\n",
      "2 --- with mean test error: 0.9790725047080979\n",
      "3 --- with mean test error: 0.982391713747646\n",
      "4 --- with mean test error: 0.9763653483992467\n",
      "5 --- with mean test error: 0.9822033898305085\n",
      "6 --- with mean test error: 0.9793785310734463\n",
      "7 --- with mean test error: 0.9770244821092279\n",
      "8 --- with mean test error: 0.9802730696798494\n",
      "9 --- with mean test error: 0.9820856873822976\n",
      "10 --- with mean test error: 0.9808851224105462\n",
      "11 --- with mean test error: 0.982909604519774\n",
      "12 --- with mean test error: 0.9803672316384181\n",
      "13 --- with mean test error: 0.9796845574387948\n",
      "14 --- with mean test error: 0.9804143126177025\n",
      "15 --- with mean test error: 0.9797787193973635\n",
      "16 --- with mean test error: 0.9804613935969868\n",
      "17 --- with mean test error: 0.9833097928436911\n",
      "18 --- with mean test error: 0.9750706214689265\n",
      "19 --- with mean test error: 0.9782250470809793\n",
      "20 --- with mean test error: 0.9746704331450095\n",
      "21 --- with mean test error: 0.9825800376647834\n",
      "22 --- with mean test error: 0.9834274952919021\n",
      "23 --- with mean test error: 0.9799905838041432\n",
      "24 --- with mean test error: 0.9827919020715631\n",
      "25 --- with mean test error: 0.982909604519774\n",
      "26 --- with mean test error: 0.9808145009416196\n",
      "27 --- with mean test error: 0.9802730696798494\n",
      "28 --- with mean test error: 0.9834510357815442\n",
      "29 --- with mean test error: 0.9811440677966102\n",
      "30 --- with mean test error: 0.9818267419962335\n",
      "31 --- with mean test error: 0.9799905838041432\n",
      "32 --- with mean test error: 0.9778483992467043\n",
      "33 --- with mean test error: 0.9813088512241055\n",
      "34 --- with mean test error: 0.9817090395480227\n",
      "35 --- with mean test error: 0.9780838041431262\n",
      "36 --- with mean test error: 0.9823446327683616\n",
      "37 --- with mean test error: 0.9788841807909605\n",
      "38 --- with mean test error: 0.9806497175141243\n",
      "39 --- with mean test error: 0.9771186440677966\n",
      "40 --- with mean test error: 0.9789077212806027\n",
      "41 --- with mean test error: 0.9808380414312619\n",
      "42 --- with mean test error: 0.9813559322033899\n",
      "43 --- with mean test error: 0.9798728813559322\n",
      "44 --- with mean test error: 0.9762241054613936\n",
      "45 --- with mean test error: 0.9826741996233522\n",
      "46 --- with mean test error: 0.9795668549905838\n",
      "47 --- with mean test error: 0.9814265536723165\n",
      "48 --- with mean test error: 0.9802966101694915\n",
      "49 --- with mean test error: 0.9779661016949153\n"
     ]
    }
   ],
   "source": [
    "def passive_learning(xtrain, ytrain, xtest, ytest):\n",
    "    settings = {'param_grid' : parameters, 'scoring' : 'f1_weighted'}\n",
    "    idx = np.arange(xtrain.shape[0])\n",
    "    used = []\n",
    "    error = []\n",
    "    \n",
    "    while len(idx) > 0:\n",
    "        # randomly get 10 index\n",
    "        np.random.shuffle(idx)\n",
    "        pool, idx = idx[:10], idx[10:]\n",
    "        # add 10 to the used index list\n",
    "        used.extend(pool)    \n",
    "        # get new training set and apply cross-validation\n",
    "        samplex, sampley = xtrain[used], ytrain.iloc[used]\n",
    "        \n",
    "        # build and fit the grid search for given classifier\n",
    "        clf = GridSearchCV(estimator=LinearSVC(penalty='l1', dual=False),**settings, cv=5).fit(samplex, sampley)\n",
    "\n",
    "        # calculate test error\n",
    "        ypred_test = clf.predict(xtest)\n",
    "        acc = accuracy_score(ytest,ypred_test)\n",
    "        error.append(acc)\n",
    "    \n",
    "    return error \n",
    "\n",
    "####################################\n",
    "passive_error = []\n",
    "passive_meanerr = []\n",
    "\n",
    "for i in range(50):\n",
    "    xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "    err = passive_learning(xtrain_norm, ytrain, xtest_norm, ytest)\n",
    "    passive_error.append(err)\n",
    "    passive_meanerr.append(mean(passive_error[i]))\n",
    "    print(i, \"--- with mean test error:\", mean(passive_error[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16635ee",
   "metadata": {},
   "source": [
    "ii.Train a SVM with a pool of 10 randomly selected data points from the training set5 using linear kernel and L1 penalty. Select the parameters of the SVM with 5-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM6 and add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all training data is used. You will have 90 SVMs that were trained using 10, 20, 30,..., 900 data points and their 90 test errors. You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a554a64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- with mean test error: 0.979967043314501\n",
      "1 --- with mean test error: 0.9802495291902072\n",
      "2 --- with mean test error: 0.9793785310734464\n",
      "3 --- with mean test error: 0.9805084745762712\n",
      "4 --- with mean test error: 0.9803201506591337\n",
      "5 --- with mean test error: 0.9814500941619586\n",
      "6 --- with mean test error: 0.9791431261770245\n",
      "7 --- with mean test error: 0.9785075329566855\n",
      "8 --- with mean test error: 0.9800141242937853\n",
      "9 --- with mean test error: 0.9759651600753296\n",
      "10 --- with mean test error: 0.9836864406779662\n",
      "11 --- with mean test error: 0.9826271186440678\n",
      "12 --- with mean test error: 0.9828389830508475\n",
      "13 --- with mean test error: 0.9813323917137476\n",
      "14 --- with mean test error: 0.9797787193973635\n",
      "15 --- with mean test error: 0.9808851224105462\n",
      "16 --- with mean test error: 0.9795197740112995\n",
      "17 --- with mean test error: 0.9828389830508475\n",
      "18 --- with mean test error: 0.9845103578154426\n",
      "19 --- with mean test error: 0.9818032015065914\n",
      "20 --- with mean test error: 0.9775659133709982\n",
      "21 --- with mean test error: 0.979590395480226\n",
      "22 --- with mean test error: 0.980861581920904\n",
      "23 --- with mean test error: 0.9791195856873823\n",
      "24 --- with mean test error: 0.9750470809792844\n",
      "25 --- with mean test error: 0.9792843691148776\n",
      "26 --- with mean test error: 0.981497175141243\n",
      "27 --- with mean test error: 0.9787429378531074\n",
      "28 --- with mean test error: 0.9824387947269304\n",
      "29 --- with mean test error: 0.9789783427495292\n",
      "30 --- with mean test error: 0.9798964218455745\n",
      "31 --- with mean test error: 0.9786723163841808\n",
      "32 --- with mean test error: 0.984557438794727\n",
      "33 --- with mean test error: 0.9760828625235405\n",
      "34 --- with mean test error: 0.9828389830508475\n",
      "35 --- with mean test error: 0.9779425612052731\n",
      "36 --- with mean test error: 0.9839689265536724\n",
      "37 --- with mean test error: 0.9788606403013183\n",
      "38 --- with mean test error: 0.9808145009416196\n",
      "39 --- with mean test error: 0.9778483992467044\n",
      "40 --- with mean test error: 0.9793549905838042\n",
      "41 --- with mean test error: 0.9820621468926554\n",
      "42 --- with mean test error: 0.9783427495291902\n",
      "43 --- with mean test error: 0.9824623352165726\n",
      "44 --- with mean test error: 0.9800141242937853\n",
      "45 --- with mean test error: 0.9808380414312619\n",
      "46 --- with mean test error: 0.9772598870056497\n",
      "47 --- with mean test error: 0.9817090395480226\n",
      "48 --- with mean test error: 0.9790960451977402\n",
      "49 --- with mean test error: 0.9842749529190208\n"
     ]
    }
   ],
   "source": [
    "def active_learning(xtrain, ytrain, xtest, ytest):\n",
    "    settings = {'param_grid' : parameters, 'scoring' : 'f1_weighted'}\n",
    "    idx = np.arange(xtrain.shape[0])\n",
    "    selected = []\n",
    "    error = []\n",
    "\n",
    "    # randomly get 10 index\n",
    "    np.random.shuffle(idx)\n",
    "    pool, idx = idx[:10], idx[10:]\n",
    "    # add 10 to the used index list\n",
    "    selected.extend(pool)    \n",
    "    \n",
    "    while len(selected) <= 900:\n",
    "        samplex, sampley = xtrain[selected], ytrain.iloc[selected]\n",
    "        clf = GridSearchCV(estimator=LinearSVC(penalty='l1', dual=False), **settings, cv=5).fit(samplex, sampley)\n",
    "        # calculate test error\n",
    "        ypred_test = clf.predict(xtest)\n",
    "        acc = accuracy_score(ytest,ypred_test)\n",
    "        error.append(acc)\n",
    "        # stop the while loop once the xtrain goes to zero\n",
    "        if len(idx) == 0:\n",
    "            break\n",
    "        # find the 10 cloeset points\n",
    "        # get the distance of data to boundary\n",
    "        dist = clf.decision_function(xtrain[idx])\n",
    "        sort = np.arange(len(idx))\n",
    "        sorted(sort, key=lambda i : abs(dist[i]))\n",
    "        idx = idx[sort]\n",
    "        # put the most closest 10 points to the pool for use, and leave the rest in place\n",
    "        pool, idx = idx[:10], idx[10:]\n",
    "        # add the pool to the already selected points list\n",
    "        selected.extend(pool)\n",
    "        \n",
    "    return error\n",
    "\n",
    "#######################################\n",
    "active_error = []\n",
    "active_meanerr = []\n",
    "\n",
    "for i in range(50):\n",
    "    xtrain_norm, xtest_norm = scaler(xtrain,xtest)\n",
    "    err = active_learning(xtrain_norm, ytrain, xtest_norm, ytest)\n",
    "    active_error.append(err)\n",
    "    active_meanerr.append(mean(active_error[i]))\n",
    "    print(i, \"--- with mean test error:\", mean(active_error[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d4252",
   "metadata": {},
   "source": [
    "(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same figure and report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "10fe91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "active = list()\n",
    "passive = list()\n",
    "\n",
    "\n",
    "for i in range(90):\n",
    "    sa = 0\n",
    "    sp = 0\n",
    "    for j in range(50):\n",
    "        sa = sa + active_error[j][i]\n",
    "        sp = sp + passive_error[j][i]\n",
    "    meana = sa/50\n",
    "    meanp = sp/50\n",
    "    active.append(meana)\n",
    "    passive.append(meanp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8340d9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = 10 * np.arange(1, 91)\n",
    "fig, axes = plt.subplots(figsize=(12, 7))\n",
    "axes.plot(x, passive, label='Passive Learning')\n",
    "axes.plot(x, active, label='Active Learning')\n",
    "axes.set_xlabel(\"Training Instance\")\n",
    "axes.set_ylabel(\"Test error\")\n",
    "axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875f968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
